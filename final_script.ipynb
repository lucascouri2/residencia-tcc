{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocessing as pp\n",
    "import load_files as lf\n",
    "import nlp_algorithms as nlp\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucas\\AppData\\Local\\Temp/ipykernel_8908/2099918564.py:1: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url1_lang</th>\n",
       "      <th>url2_lang</th>\n",
       "      <th>pair_id</th>\n",
       "      <th>link1</th>\n",
       "      <th>link2</th>\n",
       "      <th>ia_link1</th>\n",
       "      <th>ia_link2</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Entities</th>\n",
       "      <th>Time</th>\n",
       "      <th>Narrative</th>\n",
       "      <th>Overall</th>\n",
       "      <th>Style</th>\n",
       "      <th>Tone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>1484084337_1484110209</td>\n",
       "      <td>https://www.washingtonpost.com/local/virginia-man-arrested-in-fatal-dui-crash-in-west-virginia/2020/01/01/740fbc7a-2cbe-11ea-bffe-020c88b3f120_story.html</td>\n",
       "      <td>https://www.washingtonpost.com/world/the_americas/haitis-leader-marks-independence-day-amid-security-concerns/2020/01/01/dc4033a4-2cc5-11ea-bffe-020c88b3f120_story.html</td>\n",
       "      <td>https://web.archive.org/web/www.washingtonpost.com/local/virginia-man-arrested-in-fatal-dui-crash-in-west-virginia/2020/01/01/740fbc7a-2cbe-11ea-bffe-020c88b3f120_story.html</td>\n",
       "      <td>https://web.archive.org/web/www.washingtonpost.com/world/the_americas/haitis-leader-marks-independence-day-amid-security-concerns/2020/01/01/dc4033a4-2cc5-11ea-bffe-020c88b3f120_story.html</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>1484396422_1483924666</td>\n",
       "      <td>https://www.stlucianewsonline.com/guyana-three-injured-after-car-crashes-into-utility-pole/</td>\n",
       "      <td>https://www.thestar.com/news/world/europe/2020/01/01/fire-kills-animals-at-zoo-in-western-germany.html</td>\n",
       "      <td>https://web.archive.org/web/www.stlucianewsonline.com/guyana-three-injured-after-car-crashes-into-utility-pole/</td>\n",
       "      <td>https://web.archive.org/web/www.thestar.com/news/world/europe/2020/01/01/fire-kills-animals-at-zoo-in-western-germany.html</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>1.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>1484698254_1483758694</td>\n",
       "      <td>https://www.teaparty.org/trump-brings-in-2020-at-mar-a-lago-were-going-to-have-a-great-year-423052/</td>\n",
       "      <td>https://www.timesofisrael.com/trump-says-he-does-not-expect-war-with-iran-likes-peace/</td>\n",
       "      <td>https://web.archive.org/web/www.teaparty.org/trump-brings-in-2020-at-mar-a-lago-were-going-to-have-a-great-year-423052/</td>\n",
       "      <td>https://web.archive.org/web/www.timesofisrael.com/trump-says-he-does-not-expect-war-with-iran-likes-peace/</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>1576314516_1576455088</td>\n",
       "      <td>https://gadgets.ndtv.com/apps/news/zomato-uber-eats-business-acquisition-india-all-stock-deal-2167155</td>\n",
       "      <td>https://gadgets.ndtv.com/internet/news/indian-online-food-delivery-market-to-hit-usd-8-billion-by-2020-google-bcg-report-2171043</td>\n",
       "      <td>https://web.archive.org/web/gadgets.ndtv.com/apps/news/zomato-uber-eats-business-acquisition-india-all-stock-deal-2167155</td>\n",
       "      <td>https://web.archive.org/web/gadgets.ndtv.com/internet/news/indian-online-food-delivery-market-to-hit-usd-8-billion-by-2020-google-bcg-report-2171043</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>1.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>1484036253_1483894099</td>\n",
       "      <td>https://news.yahoo.com/india-approves-third-moon-mission-085759387.html</td>\n",
       "      <td>https://www.channelnewsasia.com/news/asia/india-targets-new-moon-mission-in-2020-12225344</td>\n",
       "      <td>https://web.archive.org/web/news.yahoo.com/india-approves-third-moon-mission-085759387.html</td>\n",
       "      <td>https://web.archive.org/web/www.channelnewsasia.com/news/asia/india-targets-new-moon-mission-in-2020-12225344</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  url1_lang url2_lang                pair_id  \\\n",
       "0  en        en        1484084337_1484110209   \n",
       "1  en        en        1484396422_1483924666   \n",
       "2  en        en        1484698254_1483758694   \n",
       "3  en        en        1576314516_1576455088   \n",
       "4  en        en        1484036253_1483894099   \n",
       "\n",
       "                                                                                                                                                       link1  \\\n",
       "0  https://www.washingtonpost.com/local/virginia-man-arrested-in-fatal-dui-crash-in-west-virginia/2020/01/01/740fbc7a-2cbe-11ea-bffe-020c88b3f120_story.html   \n",
       "1  https://www.stlucianewsonline.com/guyana-three-injured-after-car-crashes-into-utility-pole/                                                                 \n",
       "2  https://www.teaparty.org/trump-brings-in-2020-at-mar-a-lago-were-going-to-have-a-great-year-423052/                                                         \n",
       "3  https://gadgets.ndtv.com/apps/news/zomato-uber-eats-business-acquisition-india-all-stock-deal-2167155                                                       \n",
       "4  https://news.yahoo.com/india-approves-third-moon-mission-085759387.html                                                                                     \n",
       "\n",
       "                                                                                                                                                                      link2  \\\n",
       "0  https://www.washingtonpost.com/world/the_americas/haitis-leader-marks-independence-day-amid-security-concerns/2020/01/01/dc4033a4-2cc5-11ea-bffe-020c88b3f120_story.html   \n",
       "1  https://www.thestar.com/news/world/europe/2020/01/01/fire-kills-animals-at-zoo-in-western-germany.html                                                                     \n",
       "2  https://www.timesofisrael.com/trump-says-he-does-not-expect-war-with-iran-likes-peace/                                                                                     \n",
       "3  https://gadgets.ndtv.com/internet/news/indian-online-food-delivery-market-to-hit-usd-8-billion-by-2020-google-bcg-report-2171043                                           \n",
       "4  https://www.channelnewsasia.com/news/asia/india-targets-new-moon-mission-in-2020-12225344                                                                                  \n",
       "\n",
       "                                                                                                                                                                        ia_link1  \\\n",
       "0  https://web.archive.org/web/www.washingtonpost.com/local/virginia-man-arrested-in-fatal-dui-crash-in-west-virginia/2020/01/01/740fbc7a-2cbe-11ea-bffe-020c88b3f120_story.html   \n",
       "1  https://web.archive.org/web/www.stlucianewsonline.com/guyana-three-injured-after-car-crashes-into-utility-pole/                                                                 \n",
       "2  https://web.archive.org/web/www.teaparty.org/trump-brings-in-2020-at-mar-a-lago-were-going-to-have-a-great-year-423052/                                                         \n",
       "3  https://web.archive.org/web/gadgets.ndtv.com/apps/news/zomato-uber-eats-business-acquisition-india-all-stock-deal-2167155                                                       \n",
       "4  https://web.archive.org/web/news.yahoo.com/india-approves-third-moon-mission-085759387.html                                                                                     \n",
       "\n",
       "                                                                                                                                                                                       ia_link2  \\\n",
       "0  https://web.archive.org/web/www.washingtonpost.com/world/the_americas/haitis-leader-marks-independence-day-amid-security-concerns/2020/01/01/dc4033a4-2cc5-11ea-bffe-020c88b3f120_story.html   \n",
       "1  https://web.archive.org/web/www.thestar.com/news/world/europe/2020/01/01/fire-kills-animals-at-zoo-in-western-germany.html                                                                     \n",
       "2  https://web.archive.org/web/www.timesofisrael.com/trump-says-he-does-not-expect-war-with-iran-likes-peace/                                                                                     \n",
       "3  https://web.archive.org/web/gadgets.ndtv.com/internet/news/indian-online-food-delivery-market-to-hit-usd-8-billion-by-2020-google-bcg-report-2171043                                           \n",
       "4  https://web.archive.org/web/www.channelnewsasia.com/news/asia/india-targets-new-moon-mission-in-2020-12225344                                                                                  \n",
       "\n",
       "   Geography  Entities      Time  Narrative   Overall     Style      Tone  \n",
       "0  4.0        4.000000  1.000000  4.000000   4.000000  1.666667  2.000000  \n",
       "1  4.0        4.000000  1.000000  4.000000   3.666667  1.666667  1.333333  \n",
       "2  1.0        2.000000  1.000000  2.333333   2.333333  1.000000  1.333333  \n",
       "3  1.0        2.333333  2.666667  1.666667   2.000000  1.666667  1.666667  \n",
       "4  1.0        1.250000  1.000000  1.250000   1.250000  1.000000  1.000000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "## dfSourceV2 = pd.read_csv('../Dados/v2_semeval-2022_task8_train-data_batch.csv')\n",
    "train_v1 = pd.read_csv('dados/train v0.1.csv')\n",
    "\n",
    "train_v1_enen = train_v1[(train_v1['url1_lang'] == 'en') & (train_v1['url2_lang'] == 'en')]\n",
    "\n",
    "train_v1_enen.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'dados/train v0.1/'\n",
    "\n",
    "lista_docs = []\n",
    "lista_error = []\n",
    "lista_vazio = []\n",
    "values = train_v1_enen[['pair_id', 'Overall']]\n",
    "\n",
    "for index, values in values.iterrows():\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        json_pair = lf.get_json_document_pair(data_path, values['pair_id'])\n",
    "        text_doc1 = json_pair[0]['text']\n",
    "        text_doc2 = json_pair[1]['text']\n",
    "        \n",
    "        if ( len(text_doc1) > 0 ) and ( len(text_doc2) > 0 ):\n",
    "            lista_docs.append((values['pair_id'], text_doc1, text_doc2, values['Overall']))\n",
    "        else:\n",
    "            lista_vazio.append(values['pair_id'])\n",
    "    \n",
    "    except:\n",
    "        lista_error.append(values['pair_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text = pd.DataFrame(lista_docs,  columns=['pair_id', 'doc1', 'doc2', 'Overall'])\n",
    "#df_text.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1357, 4)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processing_list = [\n",
    "    {},\n",
    "    {\"basic_processing\": True},\n",
    "    {\"basic_processing\": True, \"no_stopwords\": True}\n",
    "    ]\n",
    "\n",
    "'''pre_processing_list = [\n",
    "    {},\n",
    "    {\"basic_processing\": True},\n",
    "    {\"basic_processing\": True, \"no_stopwords\": True},\n",
    "    {\"basic_processing\": True, \"stemming\": True},\n",
    "    {\"basic_processing\": True, \"no_stopwords\": True, \"stemming\": True},\n",
    "    {\"basic_processing\": True, \"lema\":  True},\n",
    "    {\"basic_processing\": True, \"no_stopwords\": True, \"lema\": True}]'''\n",
    "\n",
    "df_pp = pp.pre_process_all(df_text, pre_processing_list)\n",
    "#df_pp = pp.pre_process_all(df_text.iloc[:3], pre_processing_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pair_id</th>\n",
       "      <th>Overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1484084337_1484110209</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 pair_id  Overall\n",
       "0  1484084337_1484110209  4.0    "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results = df_pp[[\"pair_id\", \"Overall\"]]\n",
    "df_results.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{},\n",
       " {'basic_processing': True},\n",
       " {'basic_processing': True, 'no_stopwords': True}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_processing_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_pp.columns\n",
    "\n",
    "#df_pp[['doc2_pipeline0', 'Overall']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bert-base-nli-mean-tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'## Funcoes do encode pro fine tune\\nimport tensorflow as tf\\nimport tensorflow_addons as tfa\\n\\nfrom tensorflow.keras.optimizers import Adam\\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\\nfrom tensorflow.keras.metrics import SparseCategoricalAccuracy\\n\\nfrom transformers import TFBertForSequenceClassification\\nfrom transformers import BertTokenizer\\n\\n# can be up to 512 for BERT\\nMAX_LENGTH = 256\\nBATCH_SIZE = 64\\nSEED = 42\\nfrom transformers import BertTokenizer\\n\\ntokenizer = SentenceTransformer(\\'sentence-transformers/bert-base-nli-mean-tokens\\')\\n#BertTokenizer.from_pretrained(\\'neuralmind/bert-base-portuguese-cased\\', do_lower_case = False)\\n\\ndef map_example_to_dict(input_ids, attention_masks, token_type_ids, label):\\n  return {\\n      \"input_ids\": input_ids,\\n      \"token_type_ids\": token_type_ids,\\n      \"attention_mask\": attention_masks,\\n  }, label\\n\\ndef convert_example_to_feature(titulo_1, titulo_2):\\n    return tokenizer.encode_plus(titulo_1, titulo_2,\\n                                 add_special_tokens = True, # adiciona [CLS], [SEP]\\n                                 max_length = MAX_LENGTH, # comprimento máximo do texto de entrada\\n                                 padding = \\'max_length\\', # adiciona [PAD] até o tam_max (MAX_LENGTH)\\n                                 truncation = True, # padrão = \\'longest_first\\'\\n                                 return_attention_mask = True, # adiciona máscara de atenção para não focar nos tokens do pad\\n                                )\\n                        \\ndef encode_examples(df_titulos, labels, limit = -1):\\n    \\n    # prepare list, so that we can build up final TensorFlow dataset from slices.\\n    input_ids_list = []\\n    token_type_ids_list = []\\n    attention_mask_list = []\\n    label_list = []\\n    \\n    if (limit > 0):\\n        ds = ds.take(limit)\\n    \\n    # for review, label in tfds.as_numpy(ds):\\n    for titulo_1, titulo_2, label in zip(df_titulos[\"titulo_1\"], df_titulos[\"titulo_2\"], labels):\\n        \\n        bert_input = convert_example_to_feature(titulo_1, titulo_2)\\n        input_ids_list.append(bert_input[\\'input_ids\\'])\\n        token_type_ids_list.append(bert_input[\\'token_type_ids\\'])\\n        attention_mask_list.append(bert_input[\\'attention_mask\\'])\\n        label_list.append([label])\\n        \\n    return tf.data.Dataset.from_tensor_slices((input_ids_list, attention_mask_list, token_type_ids_list, label_list)).map(map_example_to_dict)\\n\\ndef get_bert_data(X_train, y_train, X_valid, y_valid):\\n    \\n    # train dataset\\n    ds_train = encode_examples(X_train, y_train).batch(BATCH_SIZE)\\n\\n    #validation dataset\\n    ds_valid = encode_examples(X_valid, y_valid).batch(BATCH_SIZE)\\n\\n    return ds_train, ds_valid\\n\\nimport pandas as pd\\nimport numpy as np\\nimport time\\n\\nfrom sklearn.metrics.pairwise import cosine_similarity\\n\\nfrom scipy import spatial\\nfrom sklearn.model_selection import train_test_split\\n\\n###Bert###\\ndef get_bert(model, doc1, doc2):\\n    \\n    data = [doc1, doc2]\\n    sentence_embeddings = model.encode(data)\\n\\n    infer1 = sentence_embeddings[0]\\n    infer2 = sentence_embeddings[1]\\n    \\n    cos_similarity = 1 - spatial.distance.cosine(infer1, infer2) #de 0 a 1\\n    \\n    return cos_similarity\\n\\n\\n\\ndef apply_bert(df, len_pipeline, model, model_name, fine_tune = False):\\n\\n    time_list = []\\n\\n    df_bert = pd.DataFrame()\\n    for index in range(len_pipeline):\\n        if fine_tune == True:\\n            X = df[f\\'doc1_pipeline{index}\\']\\n            y = df[\\'Overall\\']\\n            X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.2, random_state = SEED, stratify = y)\\n\\n            learning_rate = 2e-5\\n            number_of_epochs = 3\\n            ds_train, ds_valid, ds_test = get_bert_data(X_train, y_train, X_valid, y_valid)\\n            \\n            # model initialization\\n            #model = TFBertForSequenceClassification.from_pretrained(\\'neuralmind/bert-base-portuguese-cased\\', from_pt = True)\\n\\n            # choosing Adam optimizer\\n            optimizer = Adam(learning_rate=learning_rate, epsilon = 1e-08)\\n            loss = SparseCategoricalCrossentropy(from_logits = True)\\n            metric_acc = SparseCategoricalAccuracy(\\'accuracy\\')\\n            model.compile(optimizer = optimizer, loss = loss, metrics = [metric_acc])\\n\\n            #Training model\\n            bert_history = model.fit(ds_train, epochs = number_of_epochs, validation_data = ds_valid)\\n        start_time = time.time()\\n        df_bert[f\\'bert_{model_name}{index}\\'] = df.apply(lambda row: get_bert(model, \" \".join(row[f\\'doc1_pipeline{index}\\']), \" \".join(row[f\\'doc2_pipeline{index}\\'])), axis=1)\\n        time_list.append((f\\'bert_{model_name}{index}\\', time.time()-start_time))\\n\\n    return (df_bert, pd.DataFrame(time_list))'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''## Funcoes do encode pro fine tune\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
    "\n",
    "from transformers import TFBertForSequenceClassification\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# can be up to 512 for BERT\n",
    "MAX_LENGTH = 256\n",
    "BATCH_SIZE = 64\n",
    "SEED = 42\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = SentenceTransformer('sentence-transformers/bert-base-nli-mean-tokens')\n",
    "#BertTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased', do_lower_case = False)\n",
    "\n",
    "def map_example_to_dict(input_ids, attention_masks, token_type_ids, label):\n",
    "  return {\n",
    "      \"input_ids\": input_ids,\n",
    "      \"token_type_ids\": token_type_ids,\n",
    "      \"attention_mask\": attention_masks,\n",
    "  }, label\n",
    "\n",
    "def convert_example_to_feature(titulo_1, titulo_2):\n",
    "    return tokenizer.encode_plus(titulo_1, titulo_2,\n",
    "                                 add_special_tokens = True, # adiciona [CLS], [SEP]\n",
    "                                 max_length = MAX_LENGTH, # comprimento máximo do texto de entrada\n",
    "                                 padding = 'max_length', # adiciona [PAD] até o tam_max (MAX_LENGTH)\n",
    "                                 truncation = True, # padrão = 'longest_first'\n",
    "                                 return_attention_mask = True, # adiciona máscara de atenção para não focar nos tokens do pad\n",
    "                                )\n",
    "                        \n",
    "def encode_examples(df_titulos, labels, limit = -1):\n",
    "    \n",
    "    # prepare list, so that we can build up final TensorFlow dataset from slices.\n",
    "    input_ids_list = []\n",
    "    token_type_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    label_list = []\n",
    "    \n",
    "    if (limit > 0):\n",
    "        ds = ds.take(limit)\n",
    "    \n",
    "    # for review, label in tfds.as_numpy(ds):\n",
    "    for titulo_1, titulo_2, label in zip(df_titulos[\"titulo_1\"], df_titulos[\"titulo_2\"], labels):\n",
    "        \n",
    "        bert_input = convert_example_to_feature(titulo_1, titulo_2)\n",
    "        input_ids_list.append(bert_input['input_ids'])\n",
    "        token_type_ids_list.append(bert_input['token_type_ids'])\n",
    "        attention_mask_list.append(bert_input['attention_mask'])\n",
    "        label_list.append([label])\n",
    "        \n",
    "    return tf.data.Dataset.from_tensor_slices((input_ids_list, attention_mask_list, token_type_ids_list, label_list)).map(map_example_to_dict)\n",
    "\n",
    "def get_bert_data(X_train, y_train, X_valid, y_valid):\n",
    "    \n",
    "    # train dataset\n",
    "    ds_train = encode_examples(X_train, y_train).batch(BATCH_SIZE)\n",
    "\n",
    "    #validation dataset\n",
    "    ds_valid = encode_examples(X_valid, y_valid).batch(BATCH_SIZE)\n",
    "\n",
    "    return ds_train, ds_valid\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from scipy import spatial\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "###Bert###\n",
    "def get_bert(model, doc1, doc2):\n",
    "    \n",
    "    data = [doc1, doc2]\n",
    "    sentence_embeddings = model.encode(data)\n",
    "\n",
    "    infer1 = sentence_embeddings[0]\n",
    "    infer2 = sentence_embeddings[1]\n",
    "    \n",
    "    cos_similarity = 1 - spatial.distance.cosine(infer1, infer2) #de 0 a 1\n",
    "    \n",
    "    return cos_similarity\n",
    "\n",
    "\n",
    "\n",
    "def apply_bert(df, len_pipeline, model, model_name, fine_tune = False):\n",
    "\n",
    "    time_list = []\n",
    "\n",
    "    df_bert = pd.DataFrame()\n",
    "    for index in range(len_pipeline):\n",
    "        if fine_tune == True:\n",
    "            X = df[f'doc1_pipeline{index}']\n",
    "            y = df['Overall']\n",
    "            X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.2, random_state = SEED, stratify = y)\n",
    "\n",
    "            learning_rate = 2e-5\n",
    "            number_of_epochs = 3\n",
    "            ds_train, ds_valid, ds_test = get_bert_data(X_train, y_train, X_valid, y_valid)\n",
    "            \n",
    "            # model initialization\n",
    "            #model = TFBertForSequenceClassification.from_pretrained('neuralmind/bert-base-portuguese-cased', from_pt = True)\n",
    "\n",
    "            # choosing Adam optimizer\n",
    "            optimizer = Adam(learning_rate=learning_rate, epsilon = 1e-08)\n",
    "            loss = SparseCategoricalCrossentropy(from_logits = True)\n",
    "            metric_acc = SparseCategoricalAccuracy('accuracy')\n",
    "            model.compile(optimizer = optimizer, loss = loss, metrics = [metric_acc])\n",
    "\n",
    "            #Training model\n",
    "            bert_history = model.fit(ds_train, epochs = number_of_epochs, validation_data = ds_valid)\n",
    "        start_time = time.time()\n",
    "        df_bert[f'bert_{model_name}{index}'] = df.apply(lambda row: get_bert(model, \" \".join(row[f'doc1_pipeline{index}']), \" \".join(row[f'doc2_pipeline{index}'])), axis=1)\n",
    "        time_list.append((f'bert_{model_name}{index}', time.time()-start_time))\n",
    "\n",
    "    return (df_bert, pd.DataFrame(time_list))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pair_id</th>\n",
       "      <th>Overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1484084337_1484110209</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1484396422_1483924666</td>\n",
       "      <td>3.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1484698254_1483758694</td>\n",
       "      <td>2.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1576314516_1576455088</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1484036253_1483894099</td>\n",
       "      <td>1.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1352</th>\n",
       "      <td>1484430020_1483808942</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1353</th>\n",
       "      <td>1562189733_1562214022</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1354</th>\n",
       "      <td>1490711494_1500549122</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1355</th>\n",
       "      <td>1559851360_1553590552</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1356</th>\n",
       "      <td>1536407648_1541023959</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1357 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    pair_id   Overall\n",
       "0     1484084337_1484110209  4.000000\n",
       "1     1484396422_1483924666  3.666667\n",
       "2     1484698254_1483758694  2.333333\n",
       "3     1576314516_1576455088  2.000000\n",
       "4     1484036253_1483894099  1.250000\n",
       "...                     ...       ...\n",
       "1352  1484430020_1483808942  1.000000\n",
       "1353  1562189733_1562214022  1.000000\n",
       "1354  1490711494_1500549122  4.000000\n",
       "1355  1559851360_1553590552  1.000000\n",
       "1356  1536407648_1541023959  3.000000\n",
       "\n",
       "[1357 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id = df_pp['pair_id']\n",
    "overall = df_pp['Overall']\n",
    "\n",
    "pd.concat([id, overall], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from sklearn.model_selection import train_test_split\\n\\ntrain_samples = []\\nvalid_samples = []\\n\\nindex=0\\n\\nX = df_pp[[f'doc1_pipeline{index}', f'doc2_pipeline{index}']]\\ny = df_pp['Overall']\\n\\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.2, random_state = SEED)\\n\\ntrain_df = pd.concat([X_train, y_train], axis=1)\\ntrain_df = train_df.reset_index()\\nvalid_df = pd.concat([X_valid, y_valid], axis=1)\\nvalid_df = valid_df.reset_index()\\n\\n\\nfor i in range(1, len(train_df)):\\n    inp_example = InputExample(texts=[train_df.iloc[i][f'doc1_pipeline{index}'], train_df.iloc[i][f'doc2_pipeline{index}']], label=train_df.iloc[i]['Overall'])\\n    train_samples.append(inp_example)\\n\\nfor j in range(1, len(valid_df)):\\n    inp_example = InputExample(texts=[valid_df.iloc[j][f'doc1_pipeline{index}'], valid_df.iloc[j][f'doc2_pipeline{index}']], label=valid_df.iloc[j]['Overall'])\\n    valid_samples.append(inp_example)\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_samples = []\n",
    "valid_samples = []\n",
    "\n",
    "index=0\n",
    "\n",
    "X = df_pp[[f'doc1_pipeline{index}', f'doc2_pipeline{index}']]\n",
    "y = df_pp['Overall']\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.2, random_state = SEED)\n",
    "\n",
    "train_df = pd.concat([X_train, y_train], axis=1)\n",
    "train_df = train_df.reset_index()\n",
    "valid_df = pd.concat([X_valid, y_valid], axis=1)\n",
    "valid_df = valid_df.reset_index()\n",
    "\n",
    "\n",
    "for i in range(1, len(train_df)):\n",
    "    inp_example = InputExample(texts=[train_df.iloc[i][f'doc1_pipeline{index}'], train_df.iloc[i][f'doc2_pipeline{index}']], label=train_df.iloc[i]['Overall'])\n",
    "    train_samples.append(inp_example)\n",
    "\n",
    "for j in range(1, len(valid_df)):\n",
    "    inp_example = InputExample(texts=[valid_df.iloc[j][f'doc1_pipeline{index}'], valid_df.iloc[j][f'doc2_pipeline{index}']], label=valid_df.iloc[j]['Overall'])\n",
    "    valid_samples.append(inp_example)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from scipy import spatial\n",
    "\n",
    "from sentence_transformers import evaluation\n",
    "from sentence_transformers import SentenceTransformer, SentencesDataset, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "\n",
    "###Bert###\n",
    "def get_bert(model, doc1, doc2):\n",
    "    \n",
    "    data = [doc1, doc2]\n",
    "    sentence_embeddings = model.encode(data)\n",
    "\n",
    "    infer1 = sentence_embeddings[0]\n",
    "    infer2 = sentence_embeddings[1]\n",
    "    \n",
    "    cos_similarity = 1 - spatial.distance.cosine(infer1, infer2) #de 0 a 1\n",
    "    \n",
    "    return cos_similarity\n",
    "\n",
    "def apply_bert(df, len_pipeline, model, model_name, fine_tune = False):\n",
    "\n",
    "    train_batch_size = 16\n",
    "    num_epochs = 4\n",
    "    model_save_path = 'output/training_stsbenchmark_continue_training-'+model_name\n",
    "\n",
    "    time_list = []\n",
    "\n",
    "    df_bert = pd.DataFrame()\n",
    "    for index in range(len_pipeline):\n",
    "        if fine_tune==True:\n",
    "\n",
    "            train_samples = []\n",
    "            valid_samples = []\n",
    "\n",
    "            X = df[[f'doc1_pipeline{index}', f'doc2_pipeline{index}']]\n",
    "            y = df['Overall']\n",
    "\n",
    "            X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.2, random_state = SEED)\n",
    "\n",
    "            train_df = pd.concat([X_train, y_train], axis=1)\n",
    "            train_df = train_df.reset_index()\n",
    "            valid_df = pd.concat([X_valid, y_valid], axis=1)\n",
    "            valid_df = valid_df.reset_index()\n",
    "\n",
    "\n",
    "            ### TALVEZ: Na hora de montar isso aqui n possa passarr tokenizado? \n",
    "            ### Tenho q testar sem tokenizar\n",
    "            for i in range(1, len(train_df)):\n",
    "                inp_example = InputExample(texts=[train_df.iloc[i][f'doc1_pipeline{index}'], train_df.iloc[i][f'doc2_pipeline{index}']], label=float(train_df.iloc[i]['Overall']))\n",
    "                train_samples.append(inp_example)\n",
    "\n",
    "            for j in range(1, len(valid_df)):\n",
    "                inp_example = InputExample(texts=[valid_df.iloc[j][f'doc1_pipeline{index}'], valid_df.iloc[j][f'doc2_pipeline{index}']], label=float(valid_df.iloc[j]['Overall']))\n",
    "                valid_samples.append(inp_example)\n",
    "\n",
    "            train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=train_batch_size)\n",
    "            train_loss = losses.CosineSimilarityLoss(model=model)\n",
    "            evaluator = evaluation.EmbeddingSimilarityEvaluator.from_input_examples(valid_samples)#(sts_reader.get_examples('sts-dev.csv'))\n",
    "\n",
    "            model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "                    evaluator=evaluator,\n",
    "                    epochs=num_epochs,\n",
    "                    evaluation_steps=1000,\n",
    "                    #warmup_steps=warmup_steps, 100?\n",
    "                    output_path=model_save_path\n",
    "                    )\n",
    "\n",
    "        start_time = time.time()\n",
    "        df_bert[f'bert_{model_name}{index}'] = df.apply(lambda row: get_bert(model, \" \".join(row[f'doc1_pipeline{index}']), \" \".join(row[f'doc2_pipeline{index}'])), axis=1)\n",
    "        time_list.append((f'bert_{model_name}{index}', time.time()-start_time))\n",
    "\n",
    "    return (df_bert, pd.DataFrame(time_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 10)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pp[:50].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 1.18k/1.18k [00:00<00:00, 582kB/s]\n",
      "Downloading: 100%|██████████| 190/190 [00:00<00:00, 93.7kB/s]\n",
      "Downloading: 100%|██████████| 10.6k/10.6k [00:00<00:00, 2.12MB/s]\n",
      "Downloading: 100%|██████████| 571/571 [00:00<00:00, 191kB/s]\n",
      "Downloading: 100%|██████████| 116/116 [00:00<00:00, 28.8kB/s]\n",
      "Downloading: 100%|██████████| 39.3k/39.3k [00:00<00:00, 295kB/s] \n",
      "Downloading: 100%|██████████| 349/349 [00:00<00:00, 116kB/s]\n",
      "Downloading: 100%|██████████| 438M/438M [00:13<00:00, 32.2MB/s] \n",
      "Downloading: 100%|██████████| 53.0/53.0 [00:00<00:00, 13.3kB/s]\n",
      "Downloading: 100%|██████████| 239/239 [00:00<00:00, 240kB/s]\n",
      "Downloading: 100%|██████████| 466k/466k [00:00<00:00, 751kB/s] \n",
      "Downloading: 100%|██████████| 363/363 [00:00<00:00, 73.4kB/s]\n",
      "Downloading: 100%|██████████| 13.1k/13.1k [00:00<00:00, 6.69MB/s]\n",
      "Downloading: 100%|██████████| 232k/232k [00:00<00:00, 431kB/s] \n",
      "Iteration: 100%|██████████| 68/68 [01:52<00:00,  1.65s/it]\n",
      "Iteration: 100%|██████████| 68/68 [01:46<00:00,  1.57s/it]\n",
      "Iteration: 100%|██████████| 68/68 [01:52<00:00,  1.66s/it]\n",
      "Iteration: 100%|██████████| 68/68 [01:54<00:00,  1.69s/it]\n",
      "Epoch: 100%|██████████| 4/4 [07:45<00:00, 116.30s/it]\n",
      "Iteration: 100%|██████████| 68/68 [01:41<00:00,  1.49s/it]\n",
      "Iteration: 100%|██████████| 68/68 [01:45<00:00,  1.55s/it]\n",
      "Iteration: 100%|██████████| 68/68 [01:45<00:00,  1.54s/it]\n",
      "Iteration: 100%|██████████| 68/68 [01:46<00:00,  1.56s/it]\n",
      "Epoch: 100%|██████████| 4/4 [07:14<00:00, 108.69s/it]\n",
      "Iteration: 100%|██████████| 68/68 [02:27<00:00,  2.17s/it]\n",
      "Iteration: 100%|██████████| 68/68 [02:03<00:00,  1.81s/it]\n",
      "Iteration: 100%|██████████| 68/68 [02:02<00:00,  1.80s/it]\n",
      "Iteration: 100%|██████████| 68/68 [02:00<00:00,  1.77s/it]\n",
      "Epoch: 100%|██████████| 4/4 [08:54<00:00, 133.65s/it]\n"
     ]
    }
   ],
   "source": [
    "#model = SentenceTransformer('sentence-transformers/bert-base-nli-mean-tokens')\n",
    "\n",
    "#model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "model_name = 'bert_miniLM'\n",
    "\n",
    "\n",
    "(df_bert, time_bert) = apply_bert(df_pp, len(pre_processing_list), model, model_name, fine_tune=True)\n",
    "\n",
    "df_results = df_results.join(df_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bert.to_csv('./resultados/bert_minilm_ft.csv', index = False)\n",
    "time_bert.to_csv('./resultados/bert_minilm_ft_time.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Overall</th>\n",
       "      <th>bert_bert_miniLM0</th>\n",
       "      <th>bert_bert_miniLM1</th>\n",
       "      <th>bert_bert_miniLM2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Overall</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.809337</td>\n",
       "      <td>-0.794582</td>\n",
       "      <td>-0.631725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bert_bert_miniLM0</th>\n",
       "      <td>-0.809337</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.982540</td>\n",
       "      <td>0.811701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bert_bert_miniLM1</th>\n",
       "      <td>-0.794582</td>\n",
       "      <td>0.982540</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.835261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bert_bert_miniLM2</th>\n",
       "      <td>-0.631725</td>\n",
       "      <td>0.811701</td>\n",
       "      <td>0.835261</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Overall  bert_bert_miniLM0  bert_bert_miniLM1  \\\n",
       "Overall            1.000000 -0.809337          -0.794582            \n",
       "bert_bert_miniLM0 -0.809337  1.000000           0.982540            \n",
       "bert_bert_miniLM1 -0.794582  0.982540           1.000000            \n",
       "bert_bert_miniLM2 -0.631725  0.811701           0.835261            \n",
       "\n",
       "                   bert_bert_miniLM2  \n",
       "Overall           -0.631725           \n",
       "bert_bert_miniLM0  0.811701           \n",
       "bert_bert_miniLM1  0.835261           \n",
       "bert_bert_miniLM2  1.000000           "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8304/2167009006.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Overall</th>\n",
       "      <th>bert_base_mean0</th>\n",
       "      <th>bert_base_mean1</th>\n",
       "      <th>bert_base_mean2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Overall</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.813552</td>\n",
       "      <td>-0.811477</td>\n",
       "      <td>-0.774372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bert_base_mean0</th>\n",
       "      <td>-0.813552</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985662</td>\n",
       "      <td>0.941072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bert_base_mean1</th>\n",
       "      <td>-0.811477</td>\n",
       "      <td>0.985662</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.956074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bert_base_mean2</th>\n",
       "      <td>-0.774372</td>\n",
       "      <td>0.941072</td>\n",
       "      <td>0.956074</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Overall  bert_base_mean0  bert_base_mean1  bert_base_mean2\n",
       "Overall          1.000000 -0.813552        -0.811477        -0.774372       \n",
       "bert_base_mean0 -0.813552  1.000000         0.985662         0.941072       \n",
       "bert_base_mean1 -0.811477  0.985662         1.000000         0.956074       \n",
       "bert_base_mean2 -0.774372  0.941072         0.956074         1.000000       "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.corr() RESULTADO SEM FINE TUNE DO MINILM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bert-base-nli-max-tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### all-MiniLM-L6-v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USE (Universal Sentence Encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module https://tfhub.dev/google/universal-sentence-encoder/4 loaded\n"
     ]
    }
   ],
   "source": [
    "from absl import logging\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "model = hub.load(module_url)\n",
    "print (\"module %s loaded\" % module_url)\n",
    "def embed(input):\n",
    "  return model(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation coefficient = -0.7655629969170454\n",
      "p-value = 8.087010431362859e-262\n"
     ]
    }
   ],
   "source": [
    "sts_data = df_pp[['Overall', 'doc1', 'doc2']]#sts_dev\n",
    "\n",
    "def run_sts_benchmark(batch):\n",
    "  sts_encode1 = tf.nn.l2_normalize(embed(tf.constant(batch['doc1'].tolist())), axis=1)\n",
    "  sts_encode2 = tf.nn.l2_normalize(embed(tf.constant(batch['doc2'].tolist())), axis=1)\n",
    "  cosine_similarities = tf.reduce_sum(tf.multiply(sts_encode1, sts_encode2), axis=1)\n",
    "  clip_cosine_similarities = tf.clip_by_value(cosine_similarities, -1.0, 1.0)\n",
    "  scores = 1.0 - tf.acos(clip_cosine_similarities) / math.pi\n",
    "  \"\"\"Returns the similarity scores\"\"\"\n",
    "  return scores\n",
    "\n",
    "dev_scores = sts_data['Overall'].tolist()\n",
    "scores = []\n",
    "for batch in np.array_split(sts_data, 10):\n",
    "  scores.extend(run_sts_benchmark(batch))\n",
    "\n",
    "pearson_correlation = scipy.stats.pearsonr(scores, dev_scores)\n",
    "print('Pearson correlation coefficient = {0}\\np-value = {1}'.format(\n",
    "    pearson_correlation[0], pearson_correlation[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "665796ea3363072d3a6057ac2fdbe3c4fcb0d17a4b92295d9707f78e9c46c0af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
