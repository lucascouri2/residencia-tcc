{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocessing as pp\n",
    "import load_files as lf\n",
    "import nlp_algorithms as nlp\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucas\\AppData\\Local\\Temp/ipykernel_11328/2099918564.py:1: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url1_lang</th>\n",
       "      <th>url2_lang</th>\n",
       "      <th>pair_id</th>\n",
       "      <th>link1</th>\n",
       "      <th>link2</th>\n",
       "      <th>ia_link1</th>\n",
       "      <th>ia_link2</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Entities</th>\n",
       "      <th>Time</th>\n",
       "      <th>Narrative</th>\n",
       "      <th>Overall</th>\n",
       "      <th>Style</th>\n",
       "      <th>Tone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>1484084337_1484110209</td>\n",
       "      <td>https://www.washingtonpost.com/local/virginia-man-arrested-in-fatal-dui-crash-in-west-virginia/2020/01/01/740fbc7a-2cbe-11ea-bffe-020c88b3f120_story.html</td>\n",
       "      <td>https://www.washingtonpost.com/world/the_americas/haitis-leader-marks-independence-day-amid-security-concerns/2020/01/01/dc4033a4-2cc5-11ea-bffe-020c88b3f120_story.html</td>\n",
       "      <td>https://web.archive.org/web/www.washingtonpost.com/local/virginia-man-arrested-in-fatal-dui-crash-in-west-virginia/2020/01/01/740fbc7a-2cbe-11ea-bffe-020c88b3f120_story.html</td>\n",
       "      <td>https://web.archive.org/web/www.washingtonpost.com/world/the_americas/haitis-leader-marks-independence-day-amid-security-concerns/2020/01/01/dc4033a4-2cc5-11ea-bffe-020c88b3f120_story.html</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>1484396422_1483924666</td>\n",
       "      <td>https://www.stlucianewsonline.com/guyana-three-injured-after-car-crashes-into-utility-pole/</td>\n",
       "      <td>https://www.thestar.com/news/world/europe/2020/01/01/fire-kills-animals-at-zoo-in-western-germany.html</td>\n",
       "      <td>https://web.archive.org/web/www.stlucianewsonline.com/guyana-three-injured-after-car-crashes-into-utility-pole/</td>\n",
       "      <td>https://web.archive.org/web/www.thestar.com/news/world/europe/2020/01/01/fire-kills-animals-at-zoo-in-western-germany.html</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>1.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>1484698254_1483758694</td>\n",
       "      <td>https://www.teaparty.org/trump-brings-in-2020-at-mar-a-lago-were-going-to-have-a-great-year-423052/</td>\n",
       "      <td>https://www.timesofisrael.com/trump-says-he-does-not-expect-war-with-iran-likes-peace/</td>\n",
       "      <td>https://web.archive.org/web/www.teaparty.org/trump-brings-in-2020-at-mar-a-lago-were-going-to-have-a-great-year-423052/</td>\n",
       "      <td>https://web.archive.org/web/www.timesofisrael.com/trump-says-he-does-not-expect-war-with-iran-likes-peace/</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>1576314516_1576455088</td>\n",
       "      <td>https://gadgets.ndtv.com/apps/news/zomato-uber-eats-business-acquisition-india-all-stock-deal-2167155</td>\n",
       "      <td>https://gadgets.ndtv.com/internet/news/indian-online-food-delivery-market-to-hit-usd-8-billion-by-2020-google-bcg-report-2171043</td>\n",
       "      <td>https://web.archive.org/web/gadgets.ndtv.com/apps/news/zomato-uber-eats-business-acquisition-india-all-stock-deal-2167155</td>\n",
       "      <td>https://web.archive.org/web/gadgets.ndtv.com/internet/news/indian-online-food-delivery-market-to-hit-usd-8-billion-by-2020-google-bcg-report-2171043</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>1.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>1484036253_1483894099</td>\n",
       "      <td>https://news.yahoo.com/india-approves-third-moon-mission-085759387.html</td>\n",
       "      <td>https://www.channelnewsasia.com/news/asia/india-targets-new-moon-mission-in-2020-12225344</td>\n",
       "      <td>https://web.archive.org/web/news.yahoo.com/india-approves-third-moon-mission-085759387.html</td>\n",
       "      <td>https://web.archive.org/web/www.channelnewsasia.com/news/asia/india-targets-new-moon-mission-in-2020-12225344</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  url1_lang url2_lang                pair_id  \\\n",
       "0  en        en        1484084337_1484110209   \n",
       "1  en        en        1484396422_1483924666   \n",
       "2  en        en        1484698254_1483758694   \n",
       "3  en        en        1576314516_1576455088   \n",
       "4  en        en        1484036253_1483894099   \n",
       "\n",
       "                                                                                                                                                       link1  \\\n",
       "0  https://www.washingtonpost.com/local/virginia-man-arrested-in-fatal-dui-crash-in-west-virginia/2020/01/01/740fbc7a-2cbe-11ea-bffe-020c88b3f120_story.html   \n",
       "1  https://www.stlucianewsonline.com/guyana-three-injured-after-car-crashes-into-utility-pole/                                                                 \n",
       "2  https://www.teaparty.org/trump-brings-in-2020-at-mar-a-lago-were-going-to-have-a-great-year-423052/                                                         \n",
       "3  https://gadgets.ndtv.com/apps/news/zomato-uber-eats-business-acquisition-india-all-stock-deal-2167155                                                       \n",
       "4  https://news.yahoo.com/india-approves-third-moon-mission-085759387.html                                                                                     \n",
       "\n",
       "                                                                                                                                                                      link2  \\\n",
       "0  https://www.washingtonpost.com/world/the_americas/haitis-leader-marks-independence-day-amid-security-concerns/2020/01/01/dc4033a4-2cc5-11ea-bffe-020c88b3f120_story.html   \n",
       "1  https://www.thestar.com/news/world/europe/2020/01/01/fire-kills-animals-at-zoo-in-western-germany.html                                                                     \n",
       "2  https://www.timesofisrael.com/trump-says-he-does-not-expect-war-with-iran-likes-peace/                                                                                     \n",
       "3  https://gadgets.ndtv.com/internet/news/indian-online-food-delivery-market-to-hit-usd-8-billion-by-2020-google-bcg-report-2171043                                           \n",
       "4  https://www.channelnewsasia.com/news/asia/india-targets-new-moon-mission-in-2020-12225344                                                                                  \n",
       "\n",
       "                                                                                                                                                                        ia_link1  \\\n",
       "0  https://web.archive.org/web/www.washingtonpost.com/local/virginia-man-arrested-in-fatal-dui-crash-in-west-virginia/2020/01/01/740fbc7a-2cbe-11ea-bffe-020c88b3f120_story.html   \n",
       "1  https://web.archive.org/web/www.stlucianewsonline.com/guyana-three-injured-after-car-crashes-into-utility-pole/                                                                 \n",
       "2  https://web.archive.org/web/www.teaparty.org/trump-brings-in-2020-at-mar-a-lago-were-going-to-have-a-great-year-423052/                                                         \n",
       "3  https://web.archive.org/web/gadgets.ndtv.com/apps/news/zomato-uber-eats-business-acquisition-india-all-stock-deal-2167155                                                       \n",
       "4  https://web.archive.org/web/news.yahoo.com/india-approves-third-moon-mission-085759387.html                                                                                     \n",
       "\n",
       "                                                                                                                                                                                       ia_link2  \\\n",
       "0  https://web.archive.org/web/www.washingtonpost.com/world/the_americas/haitis-leader-marks-independence-day-amid-security-concerns/2020/01/01/dc4033a4-2cc5-11ea-bffe-020c88b3f120_story.html   \n",
       "1  https://web.archive.org/web/www.thestar.com/news/world/europe/2020/01/01/fire-kills-animals-at-zoo-in-western-germany.html                                                                     \n",
       "2  https://web.archive.org/web/www.timesofisrael.com/trump-says-he-does-not-expect-war-with-iran-likes-peace/                                                                                     \n",
       "3  https://web.archive.org/web/gadgets.ndtv.com/internet/news/indian-online-food-delivery-market-to-hit-usd-8-billion-by-2020-google-bcg-report-2171043                                           \n",
       "4  https://web.archive.org/web/www.channelnewsasia.com/news/asia/india-targets-new-moon-mission-in-2020-12225344                                                                                  \n",
       "\n",
       "   Geography  Entities      Time  Narrative   Overall     Style      Tone  \n",
       "0  4.0        4.000000  1.000000  4.000000   4.000000  1.666667  2.000000  \n",
       "1  4.0        4.000000  1.000000  4.000000   3.666667  1.666667  1.333333  \n",
       "2  1.0        2.000000  1.000000  2.333333   2.333333  1.000000  1.333333  \n",
       "3  1.0        2.333333  2.666667  1.666667   2.000000  1.666667  1.666667  \n",
       "4  1.0        1.250000  1.000000  1.250000   1.250000  1.000000  1.000000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "## dfSourceV2 = pd.read_csv('../Dados/v2_semeval-2022_task8_train-data_batch.csv')\n",
    "train_v1 = pd.read_csv('dados/train v0.1.csv')\n",
    "\n",
    "train_v1_enen = train_v1[(train_v1['url1_lang'] == 'en') & (train_v1['url2_lang'] == 'en')]\n",
    "\n",
    "train_v1_enen.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_v2 = pd.read_csv('dados/train v0.2.csv')\n",
    "\n",
    "train_v2_enen = train_v2[(train_v2['url1_lang'] == 'en') & (train_v2['url2_lang'] == 'en')]\n",
    "\n",
    "train_v2_enen.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('dados/eval/final_evaluation_data.csv')\n",
    "\n",
    "test_enen = test[(test['url1_lang'] == 'en') & (test['url2_lang'] == 'en')]\n",
    "\n",
    "test_enen.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_v1_enen.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_enen.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'dados/train v0.1/'\n",
    "\n",
    "lista_docs = []\n",
    "lista_error = []\n",
    "lista_vazio = []\n",
    "values = train_v1_enen[['pair_id', 'Overall']]\n",
    "\n",
    "for index, values in values.iterrows():\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        json_pair = lf.get_json_document_pair(data_path, values['pair_id'])\n",
    "        text_doc1 = json_pair[0]['text']\n",
    "        text_doc2 = json_pair[1]['text']\n",
    "        \n",
    "        if ( len(text_doc1) > 0 ) and ( len(text_doc2) > 0 ):\n",
    "            lista_docs.append((values['pair_id'], text_doc1, text_doc2, values['Overall']))\n",
    "        else:\n",
    "            lista_vazio.append(values['pair_id'])\n",
    "    \n",
    "    except:\n",
    "        lista_error.append(values['pair_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text = pd.DataFrame(lista_docs,  columns=['pair_id', 'doc1', 'doc2', 'Overall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1357, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''pre_processing_list = [\n",
    "    {},\n",
    "    {\"no_url\": True},\n",
    "    {\"basic_processing\": True},\n",
    "    {\"no_url\": True, \"basic_processing\": True},\n",
    "    {\"no_url\": True, \"basic_processing\": True, \"no_stopwords\": True}\n",
    "    ]'''\n",
    "\n",
    "pre_processing_list = [\n",
    "    {},\n",
    "    {\"no_url\": True, \"basic_processing\": True}\n",
    "    ]\n",
    "\n",
    "\n",
    "df_pp = pp.pre_process_all(df_text, pre_processing_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pair_id</th>\n",
       "      <th>Overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1484084337_1484110209</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 pair_id  Overall\n",
       "0  1484084337_1484110209  4.0    "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results = df_pp[[\"pair_id\", \"Overall\"]]\n",
    "df_results.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processing_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''## Funcoes do encode pro fine tune\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
    "\n",
    "from transformers import TFBertForSequenceClassification\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# can be up to 512 for BERT\n",
    "MAX_LENGTH = 256\n",
    "BATCH_SIZE = 64\n",
    "SEED = 42\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = SentenceTransformer('sentence-transformers/bert-base-nli-mean-tokens')\n",
    "#BertTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased', do_lower_case = False)\n",
    "\n",
    "def map_example_to_dict(input_ids, attention_masks, token_type_ids, label):\n",
    "  return {\n",
    "      \"input_ids\": input_ids,\n",
    "      \"token_type_ids\": token_type_ids,\n",
    "      \"attention_mask\": attention_masks,\n",
    "  }, label\n",
    "\n",
    "def convert_example_to_feature(titulo_1, titulo_2):\n",
    "    return tokenizer.encode_plus(titulo_1, titulo_2,\n",
    "                                 add_special_tokens = True, # adiciona [CLS], [SEP]\n",
    "                                 max_length = MAX_LENGTH, # comprimento máximo do texto de entrada\n",
    "                                 padding = 'max_length', # adiciona [PAD] até o tam_max (MAX_LENGTH)\n",
    "                                 truncation = True, # padrão = 'longest_first'\n",
    "                                 return_attention_mask = True, # adiciona máscara de atenção para não focar nos tokens do pad\n",
    "                                )\n",
    "                        \n",
    "def encode_examples(df_titulos, labels, limit = -1):\n",
    "    \n",
    "    # prepare list, so that we can build up final TensorFlow dataset from slices.\n",
    "    input_ids_list = []\n",
    "    token_type_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    label_list = []\n",
    "    \n",
    "    if (limit > 0):\n",
    "        ds = ds.take(limit)\n",
    "    \n",
    "    # for review, label in tfds.as_numpy(ds):\n",
    "    for titulo_1, titulo_2, label in zip(df_titulos[\"titulo_1\"], df_titulos[\"titulo_2\"], labels):\n",
    "        \n",
    "        bert_input = convert_example_to_feature(titulo_1, titulo_2)\n",
    "        input_ids_list.append(bert_input['input_ids'])\n",
    "        token_type_ids_list.append(bert_input['token_type_ids'])\n",
    "        attention_mask_list.append(bert_input['attention_mask'])\n",
    "        label_list.append([label])\n",
    "        \n",
    "    return tf.data.Dataset.from_tensor_slices((input_ids_list, attention_mask_list, token_type_ids_list, label_list)).map(map_example_to_dict)\n",
    "\n",
    "def get_bert_data(X_train, y_train, X_valid, y_valid):\n",
    "    \n",
    "    # train dataset\n",
    "    ds_train = encode_examples(X_train, y_train).batch(BATCH_SIZE)\n",
    "\n",
    "    #validation dataset\n",
    "    ds_valid = encode_examples(X_valid, y_valid).batch(BATCH_SIZE)\n",
    "\n",
    "    return ds_train, ds_valid\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from scipy import spatial\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "###Bert###\n",
    "def get_bert(model, doc1, doc2):\n",
    "    \n",
    "    data = [doc1, doc2]\n",
    "    sentence_embeddings = model.encode(data)\n",
    "\n",
    "    infer1 = sentence_embeddings[0]\n",
    "    infer2 = sentence_embeddings[1]\n",
    "    \n",
    "    cos_similarity = 1 - spatial.distance.cosine(infer1, infer2) #de 0 a 1\n",
    "    \n",
    "    return cos_similarity\n",
    "\n",
    "\n",
    "\n",
    "def apply_bert(df, len_pipeline, model, model_name, fine_tune = False):\n",
    "\n",
    "    time_list = []\n",
    "\n",
    "    df_bert = pd.DataFrame()\n",
    "    for index in range(len_pipeline):\n",
    "        if fine_tune == True:\n",
    "            X = df[f'doc1_pipeline{index}']\n",
    "            y = df['Overall']\n",
    "            X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.2, random_state = SEED, stratify = y)\n",
    "\n",
    "            learning_rate = 2e-5\n",
    "            number_of_epochs = 3\n",
    "            ds_train, ds_valid, ds_test = get_bert_data(X_train, y_train, X_valid, y_valid)\n",
    "            \n",
    "            # model initialization\n",
    "            #model = TFBertForSequenceClassification.from_pretrained('neuralmind/bert-base-portuguese-cased', from_pt = True)\n",
    "\n",
    "            # choosing Adam optimizer\n",
    "            optimizer = Adam(learning_rate=learning_rate, epsilon = 1e-08)\n",
    "            loss = SparseCategoricalCrossentropy(from_logits = True)\n",
    "            metric_acc = SparseCategoricalAccuracy('accuracy')\n",
    "            model.compile(optimizer = optimizer, loss = loss, metrics = [metric_acc])\n",
    "\n",
    "            #Training model\n",
    "            bert_history = model.fit(ds_train, epochs = number_of_epochs, validation_data = ds_valid)\n",
    "        start_time = time.time()\n",
    "        df_bert[f'bert_{model_name}{index}'] = df.apply(lambda row: get_bert(model, \" \".join(row[f'doc1_pipeline{index}']), \" \".join(row[f'doc2_pipeline{index}'])), axis=1)\n",
    "        time_list.append((f'bert_{model_name}{index}', time.time()-start_time))\n",
    "\n",
    "    return (df_bert, pd.DataFrame(time_list))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''id = df_pp['pair_id']\n",
    "overall = df_pp['Overall']\n",
    "\n",
    "pd.concat([id, overall], axis=1)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_samples = []\n",
    "valid_samples = []\n",
    "\n",
    "index=0\n",
    "\n",
    "X = df_pp[[f'doc1_pipeline{index}', f'doc2_pipeline{index}']]\n",
    "y = df_pp['Overall']\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.2, random_state = SEED)\n",
    "\n",
    "train_df = pd.concat([X_train, y_train], axis=1)\n",
    "train_df = train_df.reset_index()\n",
    "valid_df = pd.concat([X_valid, y_valid], axis=1)\n",
    "valid_df = valid_df.reset_index()\n",
    "\n",
    "\n",
    "for i in range(1, len(train_df)):\n",
    "    inp_example = InputExample(texts=[train_df.iloc[i][f'doc1_pipeline{index}'], train_df.iloc[i][f'doc2_pipeline{index}']], label=train_df.iloc[i]['Overall'])\n",
    "    train_samples.append(inp_example)\n",
    "\n",
    "for j in range(1, len(valid_df)):\n",
    "    inp_example = InputExample(texts=[valid_df.iloc[j][f'doc1_pipeline{index}'], valid_df.iloc[j][f'doc2_pipeline{index}']], label=valid_df.iloc[j]['Overall'])\n",
    "    valid_samples.append(inp_example)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from scipy import spatial\n",
    "\n",
    "from sentence_transformers import evaluation\n",
    "from sentence_transformers import SentenceTransformer, SentencesDataset, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "\n",
    "###Bert###\n",
    "def get_bert(model, doc1, doc2):\n",
    "    \n",
    "    data = [doc1, doc2]\n",
    "    sentence_embeddings = model.encode(data)\n",
    "\n",
    "    infer1 = sentence_embeddings[0]\n",
    "    infer2 = sentence_embeddings[1]\n",
    "    \n",
    "    cos_similarity = 1 - spatial.distance.cosine(infer1, infer2) #de 0 a 1\n",
    "    \n",
    "    return cos_similarity\n",
    "\n",
    "def apply_bert(df, len_pipeline, model, model_name, fine_tune = False):\n",
    "\n",
    "    train_batch_size = 1\n",
    "    num_epochs = 2\n",
    "    model_save_path = 'output/fine_tune_bert-'+model_name\n",
    "\n",
    "    time_list = []\n",
    "\n",
    "    df_bert = pd.DataFrame()\n",
    "    for index in range(len_pipeline):\n",
    "        if fine_tune==True:\n",
    "\n",
    "            train_samples = []\n",
    "            valid_samples = []\n",
    "\n",
    "            X = df[[f'doc1_pipeline{index}', f'doc2_pipeline{index}']]\n",
    "            y = df['Overall']\n",
    "\n",
    "            X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.2, random_state = SEED)\n",
    "\n",
    "            train_df = pd.concat([X_train, y_train], axis=1)\n",
    "            train_df = train_df.reset_index()\n",
    "            valid_df = pd.concat([X_valid, y_valid], axis=1)\n",
    "            valid_df = valid_df.reset_index()\n",
    "\n",
    "\n",
    "            ### testar sem tokenizar\n",
    "            for i in range(1, len(train_df)):\n",
    "                inp_example = InputExample(texts=[train_df.iloc[i][f'doc1_pipeline{index}'], train_df.iloc[i][f'doc2_pipeline{index}']], label=float(train_df.iloc[i]['Overall']))\n",
    "                train_samples.append(inp_example)\n",
    "\n",
    "            for j in range(1, len(valid_df)):\n",
    "                inp_example = InputExample(texts=[valid_df.iloc[j][f'doc1_pipeline{index}'], valid_df.iloc[j][f'doc2_pipeline{index}']], label=float(valid_df.iloc[j]['Overall']))\n",
    "                valid_samples.append(inp_example)\n",
    "\n",
    "            train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=train_batch_size)\n",
    "            train_loss = losses.CosineSimilarityLoss(model=model)\n",
    "            evaluator = evaluation.EmbeddingSimilarityEvaluator.from_input_examples(valid_samples)#(sts_reader.get_examples('sts-dev.csv'))\n",
    "\n",
    "            model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "                    evaluator=evaluator,\n",
    "                    epochs=num_epochs,\n",
    "                    evaluation_steps=1000,\n",
    "                    #warmup_steps=warmup_steps, 100?\n",
    "                    output_path=model_save_path\n",
    "                    )\n",
    "\n",
    "        ## ALTERAR: Avaliar no dataset de test\n",
    "        start_time = time.time()\n",
    "        df_bert[f'bert_{model_name}{index}'] = df.apply(lambda row: get_bert(model, \" \".join(row[f'doc1_pipeline{index}']), \" \".join(row[f'doc2_pipeline{index}'])), axis=1)\n",
    "        time_list.append((f'bert_{model_name}{index}', time.time()-start_time))\n",
    "\n",
    "    return (df_bert, pd.DataFrame(time_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'all-mpnet-base-v2'\n",
    "#'multi-qa-mpnet-base-dot-v1'\n",
    "#'all-distilroberta-v1'\n",
    "#'all-MiniLM-L12-v2'\n",
    "#'multi-qa-distilbert-cos-v1'\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/'+model_name)\n",
    "\n",
    "(df_bert, time_bert) = apply_bert(df_pp[:50], len(pre_processing_list), model, model_name, fine_tune=True)\n",
    "\n",
    "df_results = df_results.join(df_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bert.to_csv('./resultados/'+model_name+'.csv', index = False)\n",
    "time_bert.to_csv('./resultados/'+model_name+'.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USE (Universal Sentence Encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processing_list = [\n",
    "    {},\n",
    "    {\"no_url\": True, \"basic_processing\": True}\n",
    "    ]\n",
    "\n",
    "df_pp = pp.pre_process_all(df_text, pre_processing_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module https://tfhub.dev/google/universal-sentence-encoder/4 loaded\n"
     ]
    }
   ],
   "source": [
    "from absl import logging\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "import scipy\n",
    "\n",
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "model = hub.load(module_url)\n",
    "print (\"module %s loaded\" % module_url)\n",
    "def embed(input):\n",
    "  return model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation coefficient = -0.7655629969170454\n",
      "p-value = 8.087010431362859e-262\n"
     ]
    }
   ],
   "source": [
    "doc1_name = 'doc1'\n",
    "doc2_name = 'doc2'\n",
    "\n",
    "sts_data = df_pp[['Overall', doc1_name, doc2_name]]#sts_dev\n",
    "\n",
    "def run_sts_benchmark(batch):\n",
    "  sts_encode1 = tf.nn.l2_normalize(embed(tf.constant(batch[doc1_name].tolist())), axis=1)\n",
    "  sts_encode2 = tf.nn.l2_normalize(embed(tf.constant(batch[doc2_name].tolist())), axis=1)\n",
    "\n",
    "  cosine_similarities = tf.reduce_sum(tf.multiply(sts_encode1, sts_encode2), axis=1)\n",
    "  clip_cosine_similarities = tf.clip_by_value(cosine_similarities, -1.0, 1.0)\n",
    "  scores = 1.0 - tf.acos(clip_cosine_similarities) / math.pi\n",
    "  \"\"\"Returns the similarity scores\"\"\"\n",
    "  return scores\n",
    "\n",
    "dev_scores = sts_data['Overall'].tolist()\n",
    "scores = []\n",
    "for batch in np.array_split(sts_data, 10):\n",
    "  scores.extend(run_sts_benchmark(batch))\n",
    "\n",
    "pearson_correlation = scipy.stats.pearsonr(scores, dev_scores) #scores= predicted cos_sim, dev_scores = Overall\n",
    "print('Pearson correlation coefficient = {0}\\np-value = {1}'.format(\n",
    "    pearson_correlation[0], pearson_correlation[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1357"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processing_list = [\n",
    "    {\"tokenization\": True},\n",
    "    {\"no_url\": True, \"basic_processing\": True, \"tokenization\": True}\n",
    "    ]\n",
    "\n",
    "\n",
    "df_pp = pp.pre_process_all(df_text, pre_processing_list)\n",
    "\n",
    "(df_tf_idf, time_tf_idf) = nlp.apply_tf_idf(df_pp, len(pre_processing_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tf_idf.to_csv('./resultados/tf_idf.csv', index = False)\n",
    "time_tf_idf.to_csv('./resultados/tf_idf_time.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = df_results.join(df_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pair_id</th>\n",
       "      <th>Overall</th>\n",
       "      <th>tf_idf0</th>\n",
       "      <th>tf_idf1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1484084337_1484110209</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.117860</td>\n",
       "      <td>0.124465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1484396422_1483924666</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>0.116576</td>\n",
       "      <td>0.111068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1484698254_1483758694</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>0.312843</td>\n",
       "      <td>0.320284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1576314516_1576455088</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.415904</td>\n",
       "      <td>0.398309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1484036253_1483894099</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>0.551547</td>\n",
       "      <td>0.477166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 pair_id   Overall   tf_idf0   tf_idf1\n",
       "0  1484084337_1484110209  4.000000  0.117860  0.124465\n",
       "1  1484396422_1483924666  3.666667  0.116576  0.111068\n",
       "2  1484698254_1483758694  2.333333  0.312843  0.320284\n",
       "3  1576314516_1576455088  2.000000  0.415904  0.398309\n",
       "4  1484036253_1483894099  1.250000  0.551547  0.477166"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Overall</th>\n",
       "      <th>tf_idf0</th>\n",
       "      <th>tf_idf1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Overall</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.760434</td>\n",
       "      <td>-0.755391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tf_idf0</th>\n",
       "      <td>-0.760434</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tf_idf1</th>\n",
       "      <td>-0.755391</td>\n",
       "      <td>0.997246</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Overall   tf_idf0   tf_idf1\n",
       "Overall  1.000000 -0.760434 -0.755391\n",
       "tf_idf0 -0.760434  1.000000  0.997246\n",
       "tf_idf1 -0.755391  0.997246  1.000000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_bow, time_bow) = nlp.apply_bow(df_pp, len(pre_processing_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bow.to_csv('./resultados/bow.csv', index = False)\n",
    "time_bow.to_csv('./resultados/bow_time.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = df_results.join(df_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## countvectorizer (tfidf?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_y_pred(limite, distancia):\n",
    "\n",
    "    y_pred = [1 if num >= limite else 0 for num in distancia]\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def montar_df_resultado(y_teste, y_pred, df_teste, nome):\n",
    "    df_y = pd.DataFrame(\n",
    "                        list(zip(\n",
    "                                 y_teste, y_pred,\n",
    "                                 df_teste[\"categoria\"].to_list(),\n",
    "                                 df_teste[\"titulo_1\"].to_list(),\n",
    "                                 df_teste[\"titulo_2\"].to_list()\n",
    "                                )\n",
    "                       ), columns = ['match', 'pred', 'categoria', 'titulo_1', 'titulo_2'])\n",
    "\n",
    "    return df_y\n",
    "\n",
    "\n",
    "def salvar_distancia(y_teste, distancia, df_teste, nome):\n",
    "\n",
    "    df_y = montar_df_resultado(y_teste, distancia, df_teste, nome)\n",
    "    df_y.to_csv(f'Dados/Resultados/Cos/{nome}_distancia.csv', index = False)\n",
    "\n",
    "\n",
    "def salvar_y_pred(y_teste, y_pred, df_teste, nome, limite):\n",
    "\n",
    "    df_y = montar_df_resultado(y_teste, y_pred, df_teste, nome)\n",
    "    df_y.to_csv(f'Dados/Resultados/Cos_{limite}/Resultado/{nome}_y.csv', index = False)\n",
    "\n",
    "\n",
    "def salvar_relatorio(y_teste, y_pred, nome, tempo, limite):\n",
    "\n",
    "    relatorio = classification_report(y_teste, y_pred, output_dict = True)\n",
    "    df_relatorio = pd.DataFrame(relatorio).transpose()\n",
    "    df_relatorio['modelo'] = nome\n",
    "    df_relatorio['tempo'] = tempo\n",
    "\n",
    "    df_relatorio.to_csv(f'Dados/Resultados/Cos_{limite}/RelatÃƒÂ³rio/{nome}_relatÃƒÂ³rio.csv', index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def formatar_entrada_bow(dados, mf = 1000):\n",
    "    \n",
    "    cv = CountVectorizer(\n",
    "                         lowercase = True,\n",
    "                         strip_accents = 'unicode',\n",
    "                         max_features = mf\n",
    "                        )\n",
    "\n",
    "    cv.fit(dados)\n",
    "    dados_transformados = cv.transform(dados).toarray()\n",
    "\n",
    "    #X = matriz.fit_transform(dados).toarray()\n",
    "    \n",
    "    return cv, dados_transformados\n",
    "\n",
    "def calcular_dis_cos(vetor_1, vetor_2):\n",
    "                \n",
    "    return (1 - spatial.distance.cosine(vetor_1, vetor_2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_cos(df_teste, df_concat, tam_max):\n",
    "\n",
    "    cv, titulo_bow = formatar_entrada_bow(df_concat, mf = tam_max)\n",
    "\n",
    "    tam_df_concat = len(titulo_bow)/2\n",
    "    tam_df_teste = df_teste.shape[0]\n",
    "\n",
    "    if tam_df_concat == tam_df_teste:\n",
    "\n",
    "        distancia = []\n",
    "        for i in range( tam_df_teste ):\n",
    "            distancia.append(calcular_dis_cos(titulo_bow[i], titulo_bow[i + tam_df_teste]))\n",
    "\n",
    "        return distancia\n",
    "\n",
    "    else:\n",
    "        print (\"TAMANHO ERRADO\")\n",
    "        return \"ERRO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_cos = False\n",
    "if flag_cos == True:\n",
    "\n",
    "    for nome, df_teste in zip(arquivos, lista_df_teste):\n",
    "\n",
    "        y_teste = df_teste[\"match\"].to_list()\n",
    "\n",
    "        # remoÃƒÂ§ÃƒÂ£o de pontuaÃƒÂ§ÃƒÂ£o e acentos\n",
    "        ranking.fazer_pre_processamento(df_teste)\n",
    "\n",
    "        # colocando os titulos em um dataframe com 1 coluna sÃƒÂ³\n",
    "        df_concat = ranking.concatenar_titulos(df_teste)\n",
    "\n",
    "        # calculando o tamanho mÃƒÂ¡ximo do tÃƒÂ­tulo\n",
    "        tam_max = ranking.calcular_tam_max(df_concat)\n",
    "\n",
    "        inicio_tempo = time.time()\n",
    "        distancia = pipeline_cos(df_teste, df_concat, tam_max)\n",
    "        final_tempo = time.time()\n",
    "\n",
    "        tempo = final_tempo - inicio_tempo\n",
    "\n",
    "        salvar_distancia(y_teste, distancia, df_teste, nome)\n",
    "\n",
    "        for limite in VAR_COS:\n",
    "\n",
    "            y_pred = calcular_y_pred(limite, distancia)\n",
    "\n",
    "            salvar_y_pred(y_teste, y_pred, df_teste, nome, ranking.remove_pontuacao(str(limite)) )\n",
    "            salvar_relatorio(y_teste, y_pred, nome, tempo, ranking.remove_pontuacao(str(limite)) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "665796ea3363072d3a6057ac2fdbe3c4fcb0d17a4b92295d9707f78e9c46c0af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
