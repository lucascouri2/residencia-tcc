{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliotecas e leitura dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocessing as pp\n",
    "import load_files as lf\n",
    "import nlp_algorithms as nlp\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "train_v1 = pd.read_csv('dados/train v0.1.csv')\n",
    "\n",
    "train_v1_enen = train_v1[(train_v1['url1_lang'] == 'en') & (train_v1['url2_lang'] == 'en')]\n",
    "\n",
    "train_v1_enen.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('dados/final_evaluation_data.csv')\n",
    "\n",
    "test_enen = test[(test['url1_lang'] == 'en') & (test['url2_lang'] == 'en')]\n",
    "\n",
    "test_enen.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_v1_enen.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_enen.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leitura das URLs baixadas para cada dataset (train e test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'dados/train v0.1/'\n",
    "\n",
    "lista_docs = []\n",
    "lista_error = []\n",
    "lista_vazio = []\n",
    "values = train_v1_enen[['pair_id', 'Overall']]\n",
    "\n",
    "for index, values in values.iterrows():\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        json_pair = lf.get_json_document_pair(data_path, values['pair_id'])\n",
    "        text_doc1 = json_pair[0]['text']\n",
    "        text_doc2 = json_pair[1]['text']\n",
    "        \n",
    "        if ( len(text_doc1) > 0 ) and ( len(text_doc2) > 0 ):\n",
    "            lista_docs.append((values['pair_id'], text_doc1, text_doc2, values['Overall']))\n",
    "        else:\n",
    "            lista_vazio.append(values['pair_id'])\n",
    "    \n",
    "    except:\n",
    "        lista_error.append(values['pair_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text = pd.DataFrame(lista_docs,  columns=['pair_id', 'doc1', 'doc2', 'Overall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'dados/output_dir_test_enen/'\n",
    "\n",
    "lista_docs = []\n",
    "lista_error = []\n",
    "lista_vazio = []\n",
    "values = test_enen[['pair_id', 'Overall']]\n",
    "\n",
    "for index, values in values.iterrows():\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        json_pair = lf.get_json_document_pair(data_path, values['pair_id'])\n",
    "        text_doc1 = json_pair[0]['text']\n",
    "        text_doc2 = json_pair[1]['text']\n",
    "        \n",
    "        if ( len(text_doc1) > 0 ) and ( len(text_doc2) > 0 ):\n",
    "            lista_docs.append((values['pair_id'], text_doc1, text_doc2, values['Overall']))\n",
    "        else:\n",
    "            lista_vazio.append(values['pair_id'])\n",
    "    \n",
    "    except:\n",
    "        lista_error.append(values['pair_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text_test = pd.DataFrame(lista_docs,  columns=['pair_id', 'doc1', 'doc2', 'Overall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''pre_processing_list = [\n",
    "    {\"tokenization\": True},\n",
    "    {\"no_url\": True, \"basic_processing\": True, \"tokenization\": True}\n",
    "    ]'''\n",
    "\n",
    "pre_processing_list = [\n",
    "    {\"no_url\": True, \"basic_processing\": True, \"tokenization\": True}\n",
    "    ]\n",
    "\n",
    "df_pp = pp.pre_process_all(df_text, pre_processing_list)\n",
    "df_pp_test = pp.pre_process_all(df_text_test, pre_processing_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = df_pp_test[[\"pair_id\", \"Overall\"]]\n",
    "df_results.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processing_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_list = ['all-mpnet-base-v2', 'multi-qa-mpnet-base-dot-v1', 'all-distilroberta-v1', 'all-MiniLM-L12-v2', 'multi-qa-distilbert-cos-v1']\n",
    "\n",
    "model_list = ['all-MiniLM-L6-v2', 'multi-qa-MiniLM-L6-cos-v1', 'paraphrase-multilingual-mpnet-base-v2', 'paraphrase-albert-small-v2', 'paraphrase-multilingual-MiniLM-L12-v2', 'paraphrase-MiniLM-L3-v2', 'distiluse-base-multilingual-cased-v1', 'distiluse-base-multilingual-cased-v2']\n",
    "\n",
    "for model_name in model_list:\n",
    "    model = SentenceTransformer('sentence-transformers/'+model_name)\n",
    "\n",
    "    (df_bert, time_bert) = nlp.apply_bert(df_pp, df_pp_test, len(pre_processing_list), model, model_name, fine_tune=False)\n",
    "    df_results = df_results.join(df_bert)\n",
    "    df_bert.to_csv('./resultados/'+model_name+'.csv', index = False)\n",
    "    time_bert.to_csv('./resultados/'+model_name+'_time.csv', index = False)\n",
    "    \n",
    "df_results.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = 'all-mpnet-base-v2'\n",
    "\n",
    "model_name = 'multi-qa-mpnet-base-dot-v1'\n",
    "#model_name = 'all-distilroberta-v1'\n",
    "#model_name = 'all-MiniLM-L12-v2'\n",
    "#model_name = 'multi-qa-distilbert-cos-v1'\n",
    "#model_name = 'all-MiniLM-L6-v2'\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/'+model_name)\n",
    "\n",
    "(df_bert, time_bert) = nlp.apply_bert(df_pp, df_pp_test, len(pre_processing_list), model, model_name, fine_tune=True)\n",
    "\n",
    "df_results = df_results.join(df_bert)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bert.to_csv('./resultados/'+model_name+'.csv', index = False)\n",
    "time_bert.to_csv('./resultados/'+model_name+'_time.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USE (Universal Sentence Encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processing_list = [\n",
    "    {\"no_url\": True, \"basic_processing\": True}\n",
    "    ]\n",
    "\n",
    "df_pp = pp.pre_process_all(df_text, pre_processing_list)\n",
    "df_pp_test = pp.pre_process_all(df_text_test, pre_processing_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from absl import logging\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "import scipy\n",
    "\n",
    "#DAN encoder\n",
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "\n",
    "#Transformers based encoder \n",
    "#module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/5\"\n",
    "\n",
    "\n",
    "\n",
    "model = hub.load(module_url)\n",
    "print (\"module %s loaded\" % module_url)\n",
    "def embed(input):\n",
    "  return model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1_name = 'doc1'\n",
    "doc2_name = 'doc2'\n",
    "\n",
    "sts_data = df_pp_test[['Overall', doc1_name, doc2_name]]#sts_dev\n",
    "\n",
    "def run_sts_benchmark(batch):\n",
    "  start_time = time.time()\n",
    "  sts_encode1 = tf.nn.l2_normalize(embed(tf.constant(batch[doc1_name].tolist())), axis=1)\n",
    "  sts_encode2 = tf.nn.l2_normalize(embed(tf.constant(batch[doc2_name].tolist())), axis=1)\n",
    "\n",
    "  cosine_similarities = tf.reduce_sum(tf.multiply(sts_encode1, sts_encode2), axis=1)\n",
    "  scores = tf.clip_by_value(cosine_similarities, -1.0, 1.0)\n",
    "  #clip_cosine_similarities = tf.clip_by_value(cosine_similarities, -1.0, 1.0)\n",
    "  #scores = 1.0 - tf.acos(clip_cosine_similarities) / math.pi\n",
    "  \"\"\"Returns the similarity scores\"\"\"\n",
    "  runtime = time.time()-start_time\n",
    "  return (scores, runtime)\n",
    "\n",
    "dev_scores = sts_data['Overall'].tolist()\n",
    "scores = []\n",
    "#for batch in np.array_split(sts_data, 10):\n",
    "#  (df_use, time_use) = run_sts_benchmark(batch)\n",
    "#  scores.extend(df_use)\n",
    "\n",
    "(df_use, time_use) = run_sts_benchmark(sts_data)\n",
    "scores = df_use\n",
    "\n",
    "pearson_correlation = scipy.stats.pearsonr(scores, dev_scores) #scores= predicted cos_sim, dev_scores = Overall\n",
    "print('Pearson correlation coefficient = {0}\\np-value = {1}'.format(\n",
    "    pearson_correlation[0], pearson_correlation[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_use = pd.DataFrame(df_use.numpy()).rename(columns={0: \"USE\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = df_results.join(df_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_use = pd.DataFrame({\"time_USE\": [time_use,0]})\n",
    "#pd.DataFrame(index={time_use: 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_use.to_csv('./resultados/USE.csv', index = False)\n",
    "time_use.to_csv('./resultados/USE_time.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_results = df_pp_test[[\"pair_id\", \"Overall\"]]\n",
    "#df_results.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processing_list = [\n",
    "    {\"no_url\": True, \"basic_processing\": True, \"tokenization\": True}\n",
    "    ]\n",
    "\n",
    "\n",
    "df_pp = pp.pre_process_all(df_text, pre_processing_list)\n",
    "df_pp_test = pp.pre_process_all(df_text_test, pre_processing_list)\n",
    "\n",
    "(df_tf_idf, time_tf_idf) = nlp.apply_tf_idf(df_pp_test, len(pre_processing_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tf_idf.to_csv('./resultados/tf_idf.csv', index = False)\n",
    "time_tf_idf.to_csv('./resultados/tf_idf_time.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = df_results.join(df_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvando resultados da correlação e tempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_metodos = ['all-mpnet-base-v2', 'multi-qa-mpnet-base-dot-v1', 'all-distilroberta-v1', 'all-MiniLM-L12-v2', 'multi-qa-distilbert-cos-v1', 'all-MiniLM-L6-v2', 'multi-qa-MiniLM-L6-cos-v1', 'paraphrase-multilingual-mpnet-base-v2', 'paraphrase-albert-small-v2', 'paraphrase-multilingual-MiniLM-L12-v2', 'paraphrase-MiniLM-L3-v2', 'distiluse-base-multilingual-cased-v1', 'distiluse-base-multilingual-cased-v2', 'tf_idf', 'USE']\n",
    "df_results = df_pp_test[[\"pair_id\", \"Overall\"]]\n",
    "\n",
    "for metodo in lista_metodos:\n",
    "    df_metodo = pd.read_csv('resultados/'+metodo+'.csv')\n",
    "    df_results = df_results.join(df_metodo)\n",
    "\n",
    "df_results.corr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados_padrao = df_results.corr()['Overall']#.sort_values()\n",
    "\n",
    "resultados_sort = df_results.corr()['Overall'].sort_values()\n",
    "\n",
    "resultados_padrao.to_csv(\"resultados/corr_padrao.csv\")\n",
    "resultados_sort.to_csv(\"resultados/corr_sort.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_metodos = ['all-mpnet-base-v2', 'multi-qa-mpnet-base-dot-v1', 'all-distilroberta-v1', 'all-MiniLM-L12-v2', 'multi-qa-distilbert-cos-v1', 'all-MiniLM-L6-v2', 'multi-qa-MiniLM-L6-cos-v1', 'paraphrase-multilingual-mpnet-base-v2', 'paraphrase-albert-small-v2', 'paraphrase-multilingual-MiniLM-L12-v2', 'paraphrase-MiniLM-L3-v2', 'distiluse-base-multilingual-cased-v1', 'distiluse-base-multilingual-cased-v2', 'tf_idf', 'USE']\n",
    "df_time = pd.read_csv('resultados/'+lista_metodos[0]+'_time.csv')\n",
    "\n",
    "for metodo in lista_metodos[1:]:\n",
    "    df_metodo = pd.read_csv('resultados/'+metodo+'_time.csv')\n",
    "    df_time = df_time.append(df_metodo)\n",
    "\n",
    "df_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time.to_csv('resultados/time_padrao.csv')\n",
    "\n",
    "df_time.sort_values(by=['1']).to_csv('resultados/time_sort.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "665796ea3363072d3a6057ac2fdbe3c4fcb0d17a4b92295d9707f78e9c46c0af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
